From 84760ecd56c80c4fe2e725ea4d58a7e119a1bf89 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sat, 14 Feb 2015 10:07:35 +0800
Subject: [PATCH] bfs: priodl to speed up try_preempt().

Introduced task priodl, which first 8 bits are the task prio and the
last 56 bits are the task deadline(higher 56 bits) to speed up
try_preempt() calculation.
---
 include/linux/sched.h    |  1 +
 kernel/sched/bfs.c       | 73 +++++++++++++++++++++++++-----------------------
 kernel/sched/bfs_sched.h |  1 +
 3 files changed, 40 insertions(+), 35 deletions(-)

Index: linux-semplice-3.19.3/include/linux/sched.h
===================================================================
--- linux-semplice-3.19.3.orig/include/linux/sched.h
+++ linux-semplice-3.19.3/include/linux/sched.h
@@ -1293,6 +1293,7 @@ struct task_struct {
 #ifdef CONFIG_SCHED_BFS
 	int time_slice;
 	u64 deadline;
+	u64 priodl; /* 8bits prio and 56bits deadline for quick processing */
 	struct list_head run_list;
 	u64 last_ran;
 	u64 sched_time; /* sched_clock time spent running */
Index: linux-semplice-3.19.3/kernel/sched/bfs.c
===================================================================
--- linux-semplice-3.19.3.orig/kernel/sched/bfs.c
+++ linux-semplice-3.19.3/kernel/sched/bfs.c
@@ -517,6 +517,11 @@ static inline bool deadline_after(u64 de
 	return (deadline > time);
 }
 
+static inline void update_task_priodl(struct task_struct *p)
+{
+	p->priodl = (((u64) (p->prio))<<56) | ((p->deadline)>>8);
+}
+
 /*
  * A task that is queued but not running will be on the grq run list.
  * A task that is not running or queued will not be on the grq run list.
@@ -570,6 +575,7 @@ static void enqueue_task(struct task_str
 			p->prio = p->normal_prio;
 		else
 			p->prio = NORMAL_PRIO;
+		update_task_priodl(p);
 	}
 	__set_bit(p->prio, grq.prio_bitmap);
 	list_add_tail(&p->run_list, grq.queue + p->prio);
@@ -1303,17 +1309,9 @@ EXPORT_SYMBOL_GPL(kick_process);
  * prio PRIO_LIMIT so it is always preempted.
  */
 static inline bool
-can_preempt(struct task_struct *p, int prio, u64 deadline)
+can_preempt(struct task_struct *p, u64 priodl)
 {
-	/* Better static priority RT task or better policy preemption */
-	if (p->prio < prio)
-		return true;
-	if (p->prio > prio)
-		return false;
-	/* SCHED_NORMAL, BATCH and ISO will preempt based on deadline */
-	if (!deadline_before(p->deadline, deadline))
-		return false;
-	return true;
+	return (p->priodl < priodl);
 }
 
 #ifdef CONFIG_SMP
@@ -1331,9 +1329,9 @@ static inline bool needs_other_cpu(struc
  */
 static void try_preempt(struct task_struct *p, struct rq *this_rq)
 {
-	struct rq *highest_prio_rq = NULL;
-	int cpu, highest_prio;
-	u64 latest_deadline;
+	struct rq *rq, *highest_prio_rq = NULL;
+	int cpu;
+	u64 highest_priodl;
 	cpumask_t tmp;
 
 	/*
@@ -1353,34 +1351,29 @@ static void try_preempt(struct task_stru
 	if (unlikely(!cpumask_and(&tmp, cpu_online_mask, &p->cpus_allowed)))
 		return;
 
-	highest_prio = latest_deadline = 0;
+	cpu = cpumask_first(&tmp);
+	rq = cpu_rq(cpu);
+	highest_prio_rq = rq;
+	highest_priodl = rq->rq_priodl;
 
-	for_each_cpu_mask(cpu, tmp) {
-		struct rq *rq;
-		int rq_prio;
+	for(;cpu = cpumask_next(cpu, &tmp), cpu < nr_cpu_ids;) {
+		u64 rq_priodl;
 
 		rq = cpu_rq(cpu);
-		rq_prio = rq->rq_prio;
-		if (rq_prio < highest_prio)
-			continue;
-
-		if (rq_prio > highest_prio ||
-		    deadline_after(rq->rq_deadline, latest_deadline)) {
-			latest_deadline = rq->rq_deadline;
-			highest_prio = rq_prio;
+		rq_priodl = rq->rq_priodl;
+		if (rq_priodl > highest_priodl ) {
+			highest_priodl = rq_priodl;
 			highest_prio_rq = rq;
 		}
 	}
 
-	if (likely(highest_prio_rq)) {
 #ifdef CONFIG_SMT_NICE
-		cpu = cpu_of(highest_prio_rq);
-		if (!smt_should_schedule(p, cpu))
-			return;
+	cpu = cpu_of(highest_prio_rq);
+	if (!smt_should_schedule(p, cpu))
+		return;
 #endif
-		if (can_preempt(p, highest_prio, highest_prio_rq->rq_deadline))
-			resched_curr(highest_prio_rq);
-	}
+	if (can_preempt(p, highest_priodl))
+		resched_curr(highest_prio_rq);
 }
 #else /* CONFIG_SMP */
 static inline bool needs_other_cpu(struct task_struct *p, int cpu)
@@ -1392,7 +1385,7 @@ static void try_preempt(struct task_stru
 {
 	if (p->policy == SCHED_IDLEPRIO)
 		return;
-	if (can_preempt(p, uprq->rq_prio, uprq->rq_deadline))
+	if (can_preempt(p, uprq->rq_priodl))
 		resched_curr(uprq);
 }
 #endif /* CONFIG_SMP */
@@ -1706,6 +1699,8 @@ void wake_up_new_task(struct task_struct
 	 */
 	p->prio = rq->curr->normal_prio;
 
+	update_task_priodl(p);
+
 	activate_task(p, rq);
 	trace_sched_wakeup_new(p, 1);
 	if (unlikely(p->policy == SCHED_FIFO))
@@ -2997,6 +2992,7 @@ static void time_slice_expired(struct ta
 {
 	p->time_slice = timeslice();
 	p->deadline = grq.niffies + task_deadline_diff(p);
+	update_task_priodl(p);
 #ifdef CONFIG_SMT_NICE
 	if (!p->mm)
 		p->smt_bias = 0;
@@ -3231,6 +3227,7 @@ static inline void set_rq_task(struct rq
 	rq->rq_last_ran = p->last_ran = rq->clock_task;
 	rq->rq_policy = p->policy;
 	rq->rq_prio = p->prio;
+	rq->rq_priodl = p->priodl;
 #ifdef CONFIG_SMT_NICE
 	rq->rq_smt_bias = p->smt_bias;
 #endif
@@ -3244,6 +3241,8 @@ static void reset_rq_task(struct rq *rq,
 {
 	rq->rq_policy = p->policy;
 	rq->rq_prio = p->prio;
+	rq->rq_deadline = p->deadline;
+	rq->rq_priodl = p->priodl;
 #ifdef CONFIG_SMT_NICE
 	rq->rq_smt_bias = p->smt_bias;
 #endif
@@ -3419,7 +3418,6 @@ need_resched:
 	if (idle != prev) {
 		/* Update all the information stored on struct rq */
 		prev->time_slice = rq->rq_time_slice;
-		prev->deadline = rq->rq_deadline;
 		check_deadline(prev);
 		prev->last_ran = rq->clock_task;
 
@@ -3688,6 +3686,7 @@ void rt_mutex_setprio(struct task_struct
 	if (queued)
 		dequeue_task(p);
 	p->prio = prio;
+	update_task_priodl(p);
 	if (task_running(p) && prio > oldprio)
 		resched_task(p);
 	if (queued) {
@@ -3742,6 +3741,7 @@ void set_user_nice(struct task_struct *p
 	old_static = p->static_prio;
 	p->static_prio = new_static;
 	p->prio = effective_prio(p);
+	update_task_priodl(p);
 
 	if (queued) {
 		enqueue_task(p, rq);
@@ -4761,8 +4761,10 @@ int __sched yield_to(struct task_struct
 
 	p_rq = task_rq(p);
 	yielded = 1;
-	if (p->deadline > rq->rq_deadline)
+	if (p->deadline > rq->rq_deadline) {
 		p->deadline = rq->rq_deadline;
+		update_task_priodl(p);
+	}
 	p->time_slice += rq->rq_time_slice;
 	rq->rq_time_slice = 0;
 	if (p->time_slice > timeslice())
Index: linux-semplice-3.19.3/kernel/sched/bfs_sched.h
===================================================================
--- linux-semplice-3.19.3.orig/kernel/sched/bfs_sched.h
+++ linux-semplice-3.19.3/kernel/sched/bfs_sched.h
@@ -14,6 +14,7 @@ struct rq {
 
 	/* Stored data about rq->curr to work outside grq lock */
 	u64 rq_deadline;
+	u64 rq_priodl;
 	unsigned int rq_policy;
 	int rq_time_slice;
 	u64 rq_last_ran;
